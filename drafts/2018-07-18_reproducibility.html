---
title: "Notes on reproducible research"
author: "Remko Duursma"
date: 2018-07-18
categories: ["R"]
tags: ["reproducibility", "workflow"]
description: "Some notes on making fully reproducible workflows. Do's, don'ts, problems."
output: html_document
---



<p>It is such a nice ideal: research that can be fully reproduced by the click of a button. Wonder how those figures were made, where those p-values come from? Just look up the code repository for the article, rerun the analysis, and voila: you understand everything. In practice, however, you might run into quite a few hurdles when implementing a reproducible workflow. The web is full of guides, introductions, books, etc. - and for R users CRAN even has a <a href="https://cran.r-project.org/web/views/ReproducibleResearch.html">task view on reproducible research</a>, but usually the sticky points are avoided or ignored.</p>
<p>This post is a collection of my own notes, problems I have run into and found solutions for (or not), some do’s and don’t’s, basically anything I can think of at the time of writing. It is not intended as any sort of how-to guide or what-have-you, you can find many of those on the web (for example, from the folks at <a href="http://ropensci.github.io/reproducibility-guide/sections/introduction/">Ropensci</a>).</p>
<div id="my-reproducible-publications" class="section level1">
<h1>My reproducible publications</h1>
<p>Here is a list of papers that I have authored that are (supposedly) fully reproducible. I am only listing those where I was the first author.</p>
<p>1 - <em>Plantecophys - An R Package for Analysing and Modelling Leaf Gas Exchange Data</em>, 2015, PlosONE. [<a href="https://github.com/remkoduursma/duursma2015plosone">Code</a>] [<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0143346">Article</a>]</p>
<p>2 - <em>Leaf mass per area, not total leaf area, drives differences in above‐ground biomass distribution among woody plant functional types</em>, 2016, New Phytologist. [<a href="https://github.com/RemkoDuursma/baadanalysis">Code</a>] [<a href="https://nph.onlinelibrary.wiley.com/doi/10.1111/nph.14033">Article</a>]</p>
<p>3 - <em>Canopy leaf area of a mature evergreen Eucalyptus woodland does not respond to elevated atmospheric [CO2] but tracks water availability</em>, 2016, Global Change Biology. [<a href="https://github.com/RemkoDuursma/eucfacelaipaper">Code</a>] [<a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/gcb.13151">Article</a>]</p>
<p>4 - <em>fitplc - an R package to fit hydraulic vulnerability curves</em>, 2017, Journal of Plant Hydraulics. [<a href="https://bitbucket.org/remkoduursma/fitplcpaper">Code</a>] [<a href="https://jplanthydro.org/article/view/1541">Article</a>]</p>
<p>5 - <em>On the minimum leaf conductance: its role in models of plant water use, and ecological and environmental controls</em>, 2018, New Phytologist. [<a href="https://github.com/remkoduursma/g0paper">Code</a>] [<a href="https://nph.onlinelibrary.wiley.com/doi/10.1111/nph.15395">Article</a>]</p>
<p>For each of these papers, you should be able to download the code (in each case, a git repository), read the README.md file, and run a simple command (usually in R) to generate all figures, tables, and manuscript (as pdf or docx). The necessary data are either packaged in the repository, or (often) downloaded from remote locations.</p>
<div id="kiss" class="section level2">
<h2>1. KISS</h2>
<p>In reproducible research, you are trying to make it as easy as possible for others to run your code. The more complex you make the workflow (and the dependencies), the more likely it is that something just goes wrong. Keep It Simple, Stupid. Your workflow does not need to do more than what it is supposed to do: reproduce your analysis. This idea applies to some points below, especially (2) and (3).</p>
</div>
<div id="minimize-dependencies" class="section level2">
<h2>2. Minimize dependencies</h2>
<p>In any R-based workflow, you will be using quite many R packages. In your daily routine, who cares how many packages you load! Just do all of them. But in reproducible research, you need to keep in mind the following points:</p>
<ul>
<li>Some R package may not be available in the future (the maintainer can choose to archive a package if the demands from CRAN are just too much - yes this happens frequently).</li>
<li>Some function in an R package may change its interface, behaviour, or just be renamed. Packages change over time, and not all maintainers care very much about reverse compatibility (and in many cases, there are <a href="http://r-pkgs.had.co.nz/release.html#compatibility">good reasons to break compatibility</a>).</li>
</ul>
<p>There are tools to combat these problems (more on that below), but an important point is to simply reduce the number of packages used as much as possible. The fewer packages you use, the less likely it is that things will break. Some more points:</p>
<ul>
<li>If you use a package only for one simple function, maybe <strong>copy that function into your own code</strong> instead of loading the entire package (make sure to add some attribution though).</li>
<li><strong>Write your own functions</strong> for basic operations that perhaps exist in some package somewhere (such as a function for the standard error of the mean).</li>
<li><strong>Use packages that are used frequently</strong>, and have been around for quite some time. On CRAN, you can inspect the ‘ReverseDependencies’ field to have some indication of the usage of the package. Bonus points if the package was described in a publication, this indicates that some quality threshold was reached (and maintainers are less likely to deviate from the publication).</li>
<li><strong>Use only packages that are on CRAN</strong>. The main reason to do this is that even if the maintainer chooses to archive the package, it will still be available forever in CRAN’s archive. If you keep relying on packages hosted on github (which really should be a temporary option for developers!), these can be deleted any time. You also run into installation problems (especially on Windows), which bring more dependencies (which we hate). Because of these reasons I strictly only rely on CRAN packages.</li>
</ul>
<p>Finally, you can also <strong>use tools to set the version of the R package used</strong>, in particular <code>packrat</code> (which bloats your repository by copying all the package files, this is potentially debilitating), or <code>checkpoint</code> (which downloads the required version from Microsoft’s CRAN timemachine). But even then, less is more!</p>
</div>
<div id="b.-loading-packages" class="section level2">
<h2>2b. Loading packages</h2>
<p>Still on the topic of R packages, a simple trick that should be more common solves the following problem. Here is a script that typically loads all the packages at the top of the script:</p>
<pre class="r"><code>library(gplots)
library(geometry)
library(plantecophys)
library(rgl)

# Do stuff
# ...</code></pre>
<p>This script only works if you already did <code>install.packages</code> on each of these packages. But you don’t know which are already available until you run the script (and it fails). My favorite solution is the <code>pacman</code> package:</p>
<pre class="r"><code>if(!require(pacman))install.packages(&quot;pacman&quot;)
pacman::p_load(gplots, geometry, plantecophys, rgl)</code></pre>
<p>The <code>p_load</code> function loads the package if available, and otherwise installs it from CRAN (or Bioconductor). Of course, this does not work for the <code>pacman</code> package itself, hence we need the first line.</p>
</div>
<div id="use-object-caching-workflows-only-when-needed" class="section level2">
<h2>3. Use object-caching workflows only when needed</h2>
<p>An object-caching workflow is one where intermediate steps in an analysis are <em>cached</em> (saved) and only recomputed when any of its dependencies have changed. This way, steps that take a long time (for example, fitting models) are only executed when needed, avoiding lengthy computations when all you did is add a reference to your manuscript file.</p>
<p>This is an old idea, and has since the beginning of time been available with the <a href="https://en.wikipedia.org/wiki/Makefile">Makefile</a>. A more modern approach that can be used seemlessly with R is the <a href="https://github.com/richfitz/remake"><code>remake</code> package</a> (which I used in manuscript nr. 2 in the list above). Unfortunately development on <code>remake</code> is stagnant, but the <a href="https://github.com/ropensci/drake"><code>drake</code> package</a> is a very promising successor under active development.</p>
<p>Whatever tool you choose here - my point is simply to state that you only need these tools when at least one step in your analysis takes a painfully long time to compute. Otherwise, the effort is really, really just not worth it. In many cases you are better off to just write a script (or scripts! see point (5)) that executes all steps in succession. Again, KISS!</p>
</div>
<div id="organize-your-code" class="section level2">
<h2>4. Organize your code</h2>
<p>Yes, it is possible to cram everything into one giant script. After all, simpler is better, right? You really, really do not want to do this, and instead find a logical way to organize your repository. I have written a chapter on project management in the book ‘A Learning Guide to R’ (freely available from <a href="http://www.hiercourse.com/docs/Rnotes_20180905_web.pdf">this direct link (PDF)</a>, or from www.hiercourse.com) that discusses this topic in detail. However a few points are useful:</p>
<ul>
<li><p>Keep inputs, outputs, and code <strong>separate</strong>. This is an important point, and avoids problems with trying to figure out what is generated from where with what. I tend to have directories <code>data</code>, <code>R</code>, and <code>output</code> (and <code>output/figures</code>, <code>output/data</code> etc.).</p></li>
<li><p>Write many functions. Even if you don’t repeatedly use a function, it is useful to organize your code that way to make it very easy to understand what is being done. For example, you could have:</p></li>
</ul>
<pre class="r"><code># Collect data from online source
mydata &lt;- retrieve_data()

# Fit model
mymodel1 &lt;- fit_gam_model(mydata)</code></pre>
<p>where those two functions are defined separately (and could be very long). Avoiding long definitions helps clarify which steps you are taking in your analysis.</p>
<ul>
<li>Keep scripts (code that does things) separate from function definitions (code that only defines, but does nothing). Again, in the example above, having your functions in a script (<code>myfunctions.R</code>), and then adding the line <code>source(&quot;myfunctions.R&quot;)</code>.</li>
</ul>
</div>
<div id="use-markdown" class="section level2">
<h2>5. Use markdown</h2>
</div>
</div>
