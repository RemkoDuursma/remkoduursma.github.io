---
title: "Fuel prices in New South Wales - Part 2"
author: "Remko Duursma"
date: 2018-01-24
categories: ["R"]
tags: ["spatial", "temporal"]
description: "A closer look at the remarkable oscillation in fuel prices in Sydney"
---


```{r setup, include=FALSE, cache=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE,
               eval=TRUE, 
               out.width="600px", 
               dpi=400, 
               warning = FALSE)
```



```{r loadpackages, eval=TRUE, include=FALSE, echo=F}
pacman::p_load(dplyr, oz, magicaxis, deldir, rgeos, ggplot2, gridExtra)
library(fuelpricensw)
```


It is no secret that gasoline and diesel prices at the pump vary from day to day and from one place to the next. Few people are aware though that a typical pattern in fuel prices looks like this:


```{r, echo=FALSE}
library(ggthemes)
filter(fuel, Address == "2 Alison Rd, Randwick NSW 2031",
       FuelCode == "U91") %>% 
  ggplot(aes(Date, Price)) +
  geom_line(col="dimgrey") +
  geom_point(size=1) +
  labs(x = "", y="Price (c)") +
  theme_tufte()
```

[Newspaper articles](https://www.9news.com.au/national/2018/01/10/19/51/the-reason-why-petrol-30-cents-more-expensive-in-some-sydney-suburbs) have written about this pattern, [web services](https://www.accc.gov.au/consumers/petrol-diesel-lpg/petrol-price-cycles#petrol-prices-in-sydney) exist that show where you are on the cycle, and you can compare main cities across Australia. In Sydney, the cycle is about 30 days long (more about that later), and the timing of the spike increase in fuel (which takes only 2-3 days) varies little from pump to pump. 

I have lots of questions about this cycle! This post will take the next step in analyzing a large open source dataset on fuel prices across New South Wales. [Read my previous post](http://www.remkoduursma.com/post/2018-01-24-fuelprices1/) about obtaining and cleaning the dataset and some broad spatial patterns, and visit [this github repository](https://github.com/remkoDuursma/fuelpricensw) for an R package with the cleaned dataset (and some basic spatial attributes of the stations).

From this point on I will focus largely on service stations in the greater Sydney area. The reason is that the reporting frequency on fuel prices is higher than remote NSW - usually daily -and because there are lots of service stations in Sydney alone (>750). This analysis - that I work on in my free time - is still well in the exploratory data analysis stage. I have many ideas for follow-up analyses, modelling, and so on - but these will depend on how much time I will have to spend on this topic!

This is an R blog, so all the below will not just show results but also the code used to generate them. You can find this reproducible document by following [this direct link.](https://github.com/RemkoDuursma/remkoweb/blob/master/content/post/2018-01-24-fuelprices2.Rmd).


### The data

The data are stored in [this R package](https://github.com/remkoduursma/fuelpricensw) (not on CRAN), once installed we can do:

```{r}
library(fuelpricensw)
library(dplyr)

# Merge spatial attributes
fuel <- left_join(fuel, fuelstations, by = "Address")
```

The first step is to select all Sydney stations. I did this by drawing an awkward polygon that captures most the of more densely populated bits (because 'Greater Sydney' shapefiles include lots of national parks, many more remote areas, etc.), and deciding whether some station falls in the polygon.


```{r, message=FALSE}
library(ggmap)
library(sp)

# Map tile for background.
syd_map <- ggmap::get_map(c(lon = 151, lat=-33.8), zoom=10)

# Manually entered polygon for 'Sydney', excluding blue mountains etc.
syd_vert <- read.table(text="
                   lon lat
                   150.65 -34.1
                   150.65 -33.55
                   150.9 -33.55
                   151 -33.7
                   151.35 -33.7
                   151.27 -34.15
                   150.95 -33.95
                   150.8 -34.1",header=TRUE)


# Select stations in this polygon
in_syd <- sp::point.in.polygon(fuelstations$lon, fuelstations$lat,
                           syd_vert$lon, syd_vert$lat)
locsyd <- fuelstations[in_syd == 1,]

# Also add Brand (BP, shell, etc) to the locations dataframe.
fuelkey <- dplyr::select(fuel, Address, Brand, Postcode) %>% distinct
locsyd <- left_join(locsyd, fuelkey, by="Address")


# Sydney data
fuelsyd <- filter(fuel, Address %in% locsyd$Address)

```

A simple map shows the service stations, using the excellent `ggmap`.

```{r}
ggmap(syd_map) + 
  geom_polygon(aes(x=lon,y=lat), data=syd_vert, alpha=0.2) +
  geom_point(aes(x=lon, y=lat), data=locsyd, size=0.8, col="red") +
  labs(x="", y="")

```




### Summarizing the ups and downs in fuel prices


Next we are going to summarize the fuel price cycles into timings of 'price hikes' (the sudden increase in price), the time between cycles (time to reach the minimum counting from the maximum), the time between the minimum and the maximum, the rate of decrease of price between the cycles, and so on. 

To do this, I am going to approximate the fuel price timeseries by a saw pattern - by only considering the minimum and maximum fuel prices reached for each cycle. See the below figure for a few examples.

The code I developed for this analysis is quite long, and pretty terrible. One issue is that the data have to be cleaned in a variety of ways (short spikes in the data mess up any simple approach).

In the following I will only consider one of 11 fuel types, U91 (Unleaded petrol, 91 octane), the most commonly reported fuel in the database.

```{r}
# ... U91 only
fuelu91syd <- filter(fuelsyd, FuelCode == "U91")

# List of dataframes, one timeseries for each service station. 
# I still prefer this approach over purrr or whatever.
fuelu91syds <- split(fuelu91syd, fuelu91syd$Address)
```



```{r, echo=FALSE}

# TRUE if x[i] > x[i-1] AND x[i] > x[i + 1]
local_maximum <- function(x){
  c(NA, x[2:length(x)] > x[1:(length(x)-1)]) &
    c(x[1:(length(x)-1)] > x[2:length(x)], NA)
}

# TRUE if x[i-1] > x[i+1] AND x[i] < x[i-1] AND x[i] < x[i+1]
local_minimum <- function(x){
  c(x[2:length(x)] > x[1:(length(x)-1)], NA) & 
    c(NA, x[2:length(x)] < x[1:(length(x)-1)])
}


# Convoluted, but hey it works!
make_cycledf <- function(stationdata, plotit=FALSE, ...){
  
  zall <- stationdata[order(stationdata$DateTime),]
  
  z <- group_by(zall, Date) %>% summarize(Price = last(Price)) %>%
    as_data_frame
  
  l <- list()
  
  #incs <- c(NA, diff(z$Price))
  #z <- z[incs <= 0 | incs > 4,]
  
  # steep sudden decrease; adds noise and likely glitches
  spike_down <- which(diff(z$Price) < -10)
  if(length(spike_down))z <- z[-spike_down,]
  
  #... again seems to proper fix it
  spike_down <- which(diff(z$Price) < -10)
  if(length(spike_down))z <- z[-spike_down,]
  
  # local minimum glitches when no price change
  dzero <- which(diff(z$Price) == 0)
  if(length(dzero))z <- z[-dzero,]
  
  # price increases of less than 1ct,
  # perhaps artefact due to averaging by day?
  d <- diff(z$Price)
  dtiny <- which(d > 0 & d < 1)
  if(length(dtiny))z <- z[-dtiny,]
  
  # not sure why, but needs one more go
  d <- diff(z$Price)
  dtiny <- which(d > 0 & d < 1)
  if(length(dtiny))z <- z[-dtiny,]
  
  
  if(nrow(z) < 20)return(invisible(NULL))
  
  imax <- which(local_maximum(z$Price))
  imin <- which(local_minimum(z$Price))
  
  for(i in 1:length(imax)){
    
    # find next local minimum
    next_imin <- suppressWarnings(min(imin[imin > imax[i]]))
    if(!is.finite(next_imin))next
    prev_imin <- suppressWarnings(max(imin[imin < imax[i]]))
    if(!is.finite(prev_imin))prev_imin <- NA
    
    dat <- z[imax[i]:next_imin,]
    
    price_pre <- unique(z$Price[prev_imin])
    price_peak <- dat$Price[1]
    price_low <- dat$Price[nrow(dat)]
    date_pre <- unique(z$Date[prev_imin])
    date_peak <- dat$Date[1]
    date_low <- dat$Date[nrow(dat)]
    ndays <- as.numeric(date_low - date_peak)
    ndata <- nrow(subset(z, Date >= date_peak & Date <= date_low))
    
    l[[i]] <- data.frame(Address = unique(zall$Address),
                         price_pre = price_pre,
                         price_peak = price_peak,
                         price_hike = price_peak - price_pre,
                         price_low = price_low,
                         date_peak = date_peak,
                         date_pre = date_pre,
                         date_low = date_low,
                         ndays_cycle = ndays,
                         ndata_cycle = ndata,
                         dpricedt = (price_low - price_peak) / ndays
    )
  }
  
  out <- bind_rows(l)
  
  if(plotit){
    
    par(cex.axis=0.9)
    with(z, plot(Date, Price, pch=19, col="grey", xlab="", 
                 ylab="Price (c)",
                 axes=FALSE, ...))
    axis(2)
    axis.Date(at=seq.Date(as.Date("2016-1-1"), as.Date("2018-1-1"), by="1 month"),
              side=1, tcl=-0.15, label=FALSE)
    axis.Date(at=seq.Date(as.Date("2016-1-1"), as.Date("2018-1-1"), by="3 months"),
              side=1, tcl=-0.3, format="%b '%y")
    box()
    
    if(!is.null(out)){
      with(out, points(date_peak, price_peak, pch=21, 
                       cex=1.1, col="grey", bg="red", lwd=2))  
      with(out, points(date_pre, price_pre, pch=21, 
                       cex=1.1, col="grey",bg="blue", lwd=2))
      
      for(j in 1:nrow(out)){
        xx <- c(out$date_peak[j], out$date_low[j])
        yy <- c(out$price_peak[j], out$price_low[j])
        lines(xx,yy)
        
        xx <- c(out$date_pre[j], out$date_peak[j])
        yy <- c(out$price_pre[j], out$price_peak[j])
        lines(xx,yy, lty=5)
      }
    }
  } 
  
  return(invisible(out))
  
}
```


We then apply our custom magic function to the timeseries for each of 773 service stations in Sydney.


```{r, cache=TRUE}

# Using dplyr we can write some seriously concise code.
# Using fct_lump, combine rare brands into 'Other'
cycsyd <- lapply(fuelu91syds, make_cycledf) %>%
  bind_rows %>% 
  left_join(locsyd, by="Address") %>%
  filter(ndata_cycle > 5, price_hike > 5) %>% 
  mutate(Brand2 = forcats::fct_lump(as.factor(Brand), 6))

```

Here's one of them:

```{r, echo=FALSE}
make_cycledf(fuelu91syds[["91 Glossop St, North St Marys NSW 2760"]], 
             plotit=TRUE, ylim=c(90,140))
```

The red points are the peak prices, just after the price hike has occurred, the blue symbols the minimum price reached during this cycle - just before the price hike.

Finally I prepare a dataset with the cycles themselves summarized, simply by averaging price hikes, median prices at the minimum of the cycles, and so on.

```{r}
cycsyd_a <- group_by(cycsyd, Address) %>%
  summarize(price_hike_median = median(price_hike, na.rm=T),
            price_peak_median = median(price_peak, na.rm=T),
            price_low_median = median(price_low, na.rm=T),
            ndays_cycle_median = median(ndays_cycle),
            dpricedt = mean(dpricedt),
            lon = mean(lon), lat=mean(lat), 
            nr_5km = mean(nr_5km),
            Brand = first(Brand),
            Brand2 = first(Brand2),
            Postcode = first(Postcode)
  )

```




Now we can investigate the average 'bottom prices' (blue dots in the figure above), and the average 'peak prices' across Sydney, grouped by the different brands of service stations. Here I use the fun `ggridges` packages to make semi-overlapping grouped density plots.

```{r}
library(reshape2)
library(forcats)
library(ggridges)

brand_colours <- RColorBrewer::brewer.pal(7,"Set3")

datsub <- dplyr::select(cycsyd_a, Brand2, price_peak_median, price_low_median) %>%
  melt(id.var = "Brand2") %>%
 mutate(variable = fct_recode(variable, "peak"="price_peak_median",
                              "low"="price_low_median"),
        Brand2 = reorder(as.factor(Brand2), value, median))  
                      
ggplot(datsub, aes(y=Brand2)) +
  geom_density_ridges(aes(x=value, fill=paste(Brand2, variable)),
                      color="dimgrey", alpha=0.8) +
  theme_tufte() + 
  labs(x="Price (c)", y="") +
  ggtitle("Bottom price                                   Peak price") +
  theme(legend.position="none")  + 
  scale_fill_manual(values=rep(brand_colours, each=2))
                      
```

The results quite clearly show that Metro Fuel is the cheapest of the lot, as well as Independent, which makes up a large chunk of the 'Other' category in the figure. Prices are otherwise remarkably similar between the brands associated with large corporations. 

That's it for now. Next up is a closer look at the *timing* of price hikes. Stay tuned!















